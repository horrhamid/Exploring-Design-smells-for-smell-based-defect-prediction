{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a927a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import preprocessing\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, f_classif, RFECV, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, brier_score_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from warnings import filterwarnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_classifiers():\n",
    "    models = {\n",
    "        'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "        'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n",
    "        'LogisticRegression': LogisticRegression(),\n",
    "        'BernoulliNaiveBayes': BernoulliNB(),\n",
    "        'K-NearestNeighbor': KNeighborsClassifier(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "        'RandomForest': RandomForestClassifier(),\n",
    "        'SupportVectorMachine': SVC(),\n",
    "        'MultilayerPerceptron': MLPClassifier()\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'LinearDiscriminantAnalysis': {},\n",
    "        'QuadraticDiscriminantAnalysis': {},\n",
    "        'LogisticRegression': {'C': list(np.logspace(-4, 4, 3))},\n",
    "        'BernoulliNaiveBayes': {},\n",
    "        'K-NearestNeighbor': {},\n",
    "        'DecisionTree': {'criterion': ['gini', 'entropy'], },\n",
    "        'RandomForest': {'n_estimators': [10, 100]},\n",
    "        'SupportVectorMachine': {'C': [0.1, 100]},\n",
    "        'MultilayerPerceptron': {'hidden_layer_sizes': [(17, 8, 17)],\n",
    "                                 'activation': ['tanh', 'relu']}\n",
    "    }\n",
    "    return models, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e177b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_methods():\n",
    "    selection_methods = {\n",
    "        'chi2_20p': SelectPercentile(chi2, percentile=20),\n",
    "        'chi2_50p': SelectPercentile(chi2, percentile=50),\n",
    "        'mutual_info_classif_20p': SelectPercentile(mutual_info_classif, percentile=20),\n",
    "        'mutual_info_classif_50p': SelectPercentile(mutual_info_classif, percentile=50),\n",
    "        'f_classif_20': SelectPercentile(f_classif, percentile=20),\n",
    "        'f_classif_50': SelectPercentile(f_classif, percentile=50),\n",
    "        'recursive_elimination': RFECV(RandomForestClassifier(), min_features_to_select=3, step=1, cv=5, scoring='f1')\n",
    "    }\n",
    "    return selection_methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3768f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, params = get_all_classifiers()\n",
    "methods = get_all_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Analysis(ABC):\n",
    "    # models = {\n",
    "    #     'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis(),\n",
    "    #     'QuadraticDiscriminantAnalysis': QuadraticDiscriminantAnalysis(),\n",
    "    #     'LogisticRegression': LogisticRegression(),\n",
    "    #     'BernoulliNaiveBayes': BernoulliNB(),\n",
    "    #     'K-NearestNeighbor': KNeighborsClassifier(),\n",
    "    #     'DecisionTree': DecisionTreeClassifier(),\n",
    "    #     'RandomForest': RandomForestClassifier(),\n",
    "    #     'SupportVectorMachine': SVC(),\n",
    "    #     'MultilayerPerceptron': MLPClassifier()\n",
    "    # }\n",
    "    models = {'SupportVectorMachine': SVC()}\n",
    "\n",
    "    # params = {\n",
    "    #     'LinearDiscriminantAnalysis': {},\n",
    "    #     'QuadraticDiscriminantAnalysis': {},\n",
    "    #     'LogisticRegression': {'C': list(np.logspace(-4, 4, 3))},\n",
    "    #     'BernoulliNaiveBayes': {},\n",
    "    #     'K-NearestNeighbor': {},\n",
    "    #     'DecisionTree': {'criterion': ['gini', 'entropy'], },\n",
    "    #     'RandomForest': {'n_estimators': [10, 100]},\n",
    "    #     'SupportVectorMachine': {'C': [0.1, 100]},\n",
    "    #     'MultilayerPerceptron': {'hidden_layer_sizes': [(17, 8, 17)],\n",
    "    #                              'activation': ['tanh', 'relu']}\n",
    "    # }\n",
    "    params = {'SupportVectorMachine': {'C': [0.1, 100]}}\n",
    "\n",
    "    selection_methods = {\n",
    "        'chi2_20p': SelectPercentile(chi2, percentile=20),\n",
    "        'chi2_50p': SelectPercentile(chi2, percentile=50),\n",
    "        'mutual_info_classif_20p': SelectPercentile(mutual_info_classif, percentile=20),\n",
    "        'mutual_info_classif_50p': SelectPercentile(mutual_info_classif, percentile=50),\n",
    "        'f_classif_20': SelectPercentile(f_classif, percentile=20),\n",
    "        'f_classif_50': SelectPercentile(f_classif, percentile=50),\n",
    "        'recursive_elimination': RFECV(RandomForestClassifier(), min_features_to_select=3, step=1, cv=5, scoring='f1')\n",
    "    }\n",
    "    # selection_methods = {}\n",
    "\n",
    "    def __init__(self, log_name, project: ProjectName, metric: str):\n",
    "        self.logs = Logs(log_name)\n",
    "        self.project = project\n",
    "        self.project_name = project.github()\n",
    "        self.metric = metric\n",
    "        self.versions = self._get_versions()\n",
    "        self.caching = Caching(self.project_name, self.metric, log_name)\n",
    "\n",
    "    def set_classifiers(self, models, params):\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        return self\n",
    "\n",
    "    def set_selection_methods(self, methods):\n",
    "        self.selection_methods = methods\n",
    "        return self\n",
    "\n",
    "    def _get_versions(self):\n",
    "        self.logs.general(\"{0} | {1} | 1/11 | Getting Versions ...\".format(self.metric, self.project_name))\n",
    "        versions_dir = Config.get_work_dir_path(os.path.join(\"paper\", \"versions\"))\n",
    "        versions_path = os.path.join(versions_dir, self.project_name + \".csv\")\n",
    "        versions = pd.read_csv(versions_path)['version'].to_list()\n",
    "        self.logs.success(\"{0} | {1} | 1/11 | Got Versions.\".format(self.metric, self.project_name))\n",
    "        return versions\n",
    "\n",
    "    def analyse(self):\n",
    "        try:\n",
    "            datasets = self.build_datasets(self.versions)\n",
    "            training_df, testing_df = self.split_dataset(datasets)\n",
    "            training_df, testing_df = self.handle_missing_values(training_df, testing_df)\n",
    "            selected_features, selected_training = self.select_features(training_df)\n",
    "            oversampled_training = self.oversample(selected_training, training_df)\n",
    "            selected_testing = self.get_selected_testing(testing_df, selected_features)\n",
    "            summaries = self.hyper_parameterize(oversampled_training)\n",
    "            top_summaries = self.get_top_summaries(summaries)\n",
    "            configurations = self.get_configurations(top_summaries)\n",
    "            self.calculate_scores(configurations, oversampled_training, selected_testing)\n",
    "            self.logs.summary(\"{0} | {1} | project succeeded.\".format(self.metric, self.project_name))\n",
    "\n",
    "        except Analysis.FailedBuildDataset:\n",
    "            self.logs.failure(\"{0} | {1} | 2/11 | Failed BUILDING dataset\".format(self.metric, self.project_name),\n",
    "                              verbose=True)\n",
    "            self.logs.summary(\"{0} | {1} | project failed.\".format(self.metric, self.project_name))\n",
    "            return\n",
    "\n",
    "        except Analysis.FailedSplit:\n",
    "            self.logs.failure(\"{0} | {1} | 3/11 | There are missing datasets\".format(self.metric, self.project_name))\n",
    "            self.logs.summary(\"{0} | {1} | project failed.\".format(self.metric, self.project_name))\n",
    "            return\n",
    "\n",
    "        except:\n",
    "            self.logs.failure(\"{0} | {1} | Failed to analyse project\".format(self.metric, self.project_name),\n",
    "                              verbose=True)\n",
    "            self.logs.summary(\"{0} | {1} | project failed.\".format(self.metric, self.project_name))\n",
    "            return\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_datasets(self, versions):\n",
    "        self.logs.general(\"{0} | {1} | 2/11 | Building Datasets ...\".format(self.metric, self.project_name))\n",
    "        pass\n",
    "\n",
    "    def split_dataset(self, datasets):\n",
    "        self.logs.general(\"{0} | {1} | 3/11 | Splitting dataset ...\".format(self.metric, self.project_name))\n",
    "        if any(dataset is None for dataset in datasets):\n",
    "            raise Analysis.FailedSplit()\n",
    "        training_df = pd.concat(datasets[:-1], ignore_index=True).drop([\"File\", \"Class\"], axis=1)\n",
    "        testing_df = datasets[-1].drop([\"File\", \"Class\"], axis=1)\n",
    "        self.caching.store_datasets(training_df, testing_df)\n",
    "        self.logs.success(\n",
    "            \"{0} | {1} | 3/11 | Splitted training and testing datasets.\".format(self.metric, self.project_name))\n",
    "        return training_df, testing_df\n",
    "\n",
    "    class FailedSplit(Exception):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def handle_missing_values(self, training_df, testing_df):\n",
    "        self.logs.general(\"{0} | {1} | 4/11 | Handling missing values\".format(self.metric, self.project_name))\n",
    "        pass\n",
    "\n",
    "    def select_features(self, training_df):\n",
    "        self.logs.general(\"{0} | {1} | 5/11 | Selecting Features ...\".format(self.metric, self.project_name))\n",
    "        dataset = pd.DataFrame.copy(training_df)\n",
    "        y = dataset.pop('Bugged').values\n",
    "        X = dataset.values\n",
    "        features = dataset.columns\n",
    "        selector = FeatureSelectionHelper(self.selection_methods, features)\n",
    "        selector.select(X, y)\n",
    "        selected_features = selector.get_selected_features()\n",
    "        selected_dataset = selector.get_selected_dataset()\n",
    "        self.caching.store_selected_features(selected_features, selected_dataset)\n",
    "        self.logs.success(\"{0} | {1} | 5/11 | Selected Versions.\".format(self.metric, self.project_name))\n",
    "        return selected_features, selected_dataset\n",
    "\n",
    "    def oversample(self, selected_datasets, training_df):\n",
    "        self.logs.general(\"{0} | {1} | 6/11 | Oversampling dataset ...\".format(self.metric, self.project_name))\n",
    "        y = training_df['Bugged'].values\n",
    "        oversampled_datasets = {method: SMOTE().fit_resample(X, y) for method, X in selected_datasets.items()}\n",
    "        self.caching.store_oversamples(oversampled_datasets)\n",
    "        self.logs.success(\"{0} | {1} | 6/11 | Oversampled dataset.\".format(self.metric, self.project_name))\n",
    "        return oversampled_datasets\n",
    "\n",
    "    def hyper_parameterize(self, oversample_datasets):\n",
    "        def get_summary(X, y):\n",
    "            helper = EstimatorSelectionHelper(self.models, self.params)\n",
    "            helper.fit(X, y)\n",
    "            return helper.score_summary()\n",
    "\n",
    "        self.logs.general(\"{0} | {1} | 7/11 | Tuning models and parameters ...\".format(self.metric, self.project_name))\n",
    "        summaries = {method: get_summary(data[0], data[1])\n",
    "                     for method, data in oversample_datasets.items()}\n",
    "        self.caching.store_summaries(summaries)\n",
    "        self.logs.success(\"{0} | {1} | 7/11 | Tuned models and parameters.\".format(self.metric, self.project_name))\n",
    "        return summaries\n",
    "\n",
    "    def get_top_summaries(self, summaries, n=10):\n",
    "        self.logs.general(\"{0} | {1} | 8/11 | Getting Top Summaries ...\".format(self.metric, self.project_name))\n",
    "        top_summaries = {method: summary[:n] for method, summary in summaries.items()}\n",
    "        self.caching.store_top_summaries(top_summaries)\n",
    "        self.logs.success(\"{0} | {1} | 8/11 | Got Top Summaries.\".format(self.metric, self.project_name))\n",
    "        return top_summaries\n",
    "\n",
    "    def get_configurations(self, top_summaries):\n",
    "        self.logs.general(\"{0} | {1} | 9/11 | Getting Configurations ...\".format(self.metric, self.project_name))\n",
    "        configurations = {method: list(map(lambda x: x[1].to_dict(),\n",
    "                                           top_summary.drop(EstimatorSelectionHelper.get_scores_info(),\n",
    "                                                            axis=1)\n",
    "                                           .where(pd.notnull(top_summary), None).iterrows()))\n",
    "                          for method, top_summary in top_summaries.items()}\n",
    "        self.logs.success(\"{0} | {1} | 9/11 | Got Configurations.\".format(self.metric, self.project_name))\n",
    "        return configurations\n",
    "\n",
    "    def get_selected_testing(self, testing_df, selected_features):\n",
    "        self.logs.general(\"{0} | {1} | 10/11 | Get Selected Testing Dataset ...\".format(self.metric, self.project_name))\n",
    "        testing_y = testing_df.pop('Bugged').values\n",
    "        selected_testing_datasets = {\n",
    "            method: (testing_df[testing_df.columns.intersection(features)].values, testing_y)\n",
    "            for method, features in selected_features.items()\n",
    "        }\n",
    "        self.caching.store_selected_testing_datasets(selected_testing_datasets)\n",
    "        self.logs.success(\"{0} | {1} | 10/11 | Got Selected Testing Dataset.\".format(self.metric, self.project_name))\n",
    "        return selected_testing_datasets\n",
    "\n",
    "    def calculate_scores(self, configurations, oversampled_training, selected_testing):\n",
    "        def calculate_score(method_name, training, testing, configuration):\n",
    "            estimator = self.models[configuration['estimator']]\n",
    "            params = {key: val for key, val in configuration.items() if not (val is None or key == 'estimator')}\n",
    "            estimator.set_params(**params)\n",
    "            training_X, training_y = training\n",
    "            estimator.fit(training_X, training_y)\n",
    "            testing_X, testing_y = testing\n",
    "            prediction_y = estimator.predict(testing_X)\n",
    "            scores_dict = {\n",
    "                'estimator': configuration['estimator'],\n",
    "                'configuration': str(params),\n",
    "                'feature_selection': method_name,\n",
    "                'precision': precision_score(testing_y, prediction_y),\n",
    "                'recall': recall_score(testing_y, prediction_y),\n",
    "                'f1-measure': f1_score(testing_y, prediction_y),\n",
    "                'auc-roc': roc_auc_score(testing_y, prediction_y),\n",
    "                'brier score': brier_score_loss(testing_y, prediction_y)\n",
    "            }\n",
    "            return scores_dict\n",
    "\n",
    "        self.logs.general(\"{0} | {1} | 11/11 | Calculate Scores ...\".format(self.metric, self.project_name))\n",
    "        method_names = configurations.keys()\n",
    "        scores_dicts = list(map(lambda method_name:\n",
    "                                list(map(lambda configuration:\n",
    "                                         calculate_score(method_name,\n",
    "                                                         oversampled_training[method_name],\n",
    "                                                         selected_testing[method_name],\n",
    "                                                         configuration),\n",
    "                                         configurations[method_name])), method_names))\n",
    "        scores_df = [pd.DataFrame(score) for score in scores_dicts]\n",
    "        scores = pd.concat(scores_df)\n",
    "        self.caching.store_scores(scores)\n",
    "        self.logs.success(\"{0} | {1} | 11/11 | Calculated Scores.\".format(self.metric, self.project_name))\n",
    "        return scores\n",
    "\n",
    "    class FailedBuildDataset(Exception):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Designite(Analysis):\n",
    "\n",
    "    def build_datasets(self, versions):\n",
    "        def build_dataset(version):\n",
    "            db = Builders.get_designite_builder(self.project, version)\n",
    "            classes_df, methods_df = db.build()\n",
    "            if classes_df.empty:\n",
    "                raise Analysis.FailedBuildDataset(\"Designite Smells dataset is empty.\")\n",
    "            return classes_df\n",
    "\n",
    "        super().build_datasets(versions)\n",
    "        datasets = list(map(build_dataset, versions))\n",
    "        self.logs.success(\"{0} | {1} | 2/11 | Built Datasets.\".format(self.metric, self.project_name))\n",
    "        return datasets\n",
    "\n",
    "    def handle_missing_values(self, training_df, testing_df):\n",
    "        super().handle_missing_values(training_df, testing_df)\n",
    "        training_df = training_df.dropna().astype(int)\n",
    "        testing_df = testing_df.dropna().astype(int)\n",
    "        self.logs.success(\"{0} | {1} | 4/11 | Handling missing values.\".format(self.metric, self.project_name))\n",
    "        return training_df, testing_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
